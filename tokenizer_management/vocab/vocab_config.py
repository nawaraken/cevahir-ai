from typing import Dict
from tokenizer_management.config import TOKENIZER_CONFIG, TOKEN_MAPPING

class VocabConfig:
    """
    Ã–zel token yapÄ±landÄ±rmasÄ±nÄ± tutan yapÄ±.
    """

    @classmethod
    def get_special_tokens(cls) -> Dict[str, str]:
        """
        Ã–zel token tanÄ±mlarÄ±nÄ± dÃ¶ner.

        Returns:
            Dict[str, str]: Ã–zel token eÅŸleÅŸmelerini dÃ¶ner.
        """
        return {
            "padding_token": TOKENIZER_CONFIG["padding_token"],
            "unknown_token": TOKENIZER_CONFIG["unknown_token"],
            "start_token": TOKENIZER_CONFIG["start_token"],
            "end_token": TOKENIZER_CONFIG["end_token"],
        }
    
    @classmethod
    def get_special_token_ids(cls) -> Dict[str, int]:
        """
        Ã–zel token ID eÅŸleÅŸmelerini dÃ¶ner.

        Returns:
            Dict[str, int]: Ã–zel token ID eÅŸleÅŸmelerini dÃ¶ner.
        """
        special_tokens = cls.get_special_tokens()
        token_ids = {}
        for token_name, token in special_tokens.items():
            if token in TOKEN_MAPPING:
                token_ids[token] = TOKEN_MAPPING[token]
            else:
                raise KeyError(f"Token mapping bulunamadÄ±: {token_name} -> {token}")
        return token_ids
    
    @classmethod
    def get_vocab_settings(cls) -> Dict[str, int]:
        """
        Vocab yapÄ±landÄ±rmasÄ±nÄ± dÃ¶ner.

        Returns:
            Dict[str, int]: Vocab yapÄ±landÄ±rma detaylarÄ±nÄ± dÃ¶ner.
        """
        return {
            "vocab_size": TOKENIZER_CONFIG["vocab_size"],
            "max_seq_length": TOKENIZER_CONFIG["max_seq_length"],
        }

    @classmethod
    def validate_special_tokens(cls) -> bool:
        """
        Ã–zel tokenlarÄ±n token mapping ile uyumunu doÄŸrular.

        Returns:
            bool: TÃ¼m eÅŸleÅŸmeler doÄŸruysa True, deÄŸilse False.
        """
        try:
            special_tokens = cls.get_special_tokens()
            token_ids = cls.get_special_token_ids()
            for token in special_tokens.values():
                if token not in TOKEN_MAPPING:
                    raise KeyError(f"EÅŸleÅŸmeyen token: {token}")
            return True
        except KeyError as e:
            print(f"[!] Ã–zel token hatasÄ±: {e}")
            return False


# === Test AmaÃ§lÄ± Ã‡alÄ±ÅŸtÄ±rma ===
if __name__ == "__main__":
    print("\nğŸ” Ã–zel Tokenlar:")
    print(VocabConfig.get_special_tokens())

    print("\nğŸ” Ã–zel Token ID'leri:")
    print(VocabConfig.get_special_token_ids())

    print("\nğŸ” Vocab YapÄ±landÄ±rmasÄ±:")
    print(VocabConfig.get_vocab_settings())

    print("\nğŸ” EÅŸleÅŸme DoÄŸrulamasÄ±:")
    if VocabConfig.validate_special_tokens():
        print("[âœ”ï¸] TÃ¼m token eÅŸleÅŸmeleri doÄŸru!")
    else:
        print("[âŒ] Token eÅŸleÅŸmelerinde hata var!")
