import os
import json
import pytest
from tokenizer_management.vocab.vocab_builder import VocabBuilder, VocabBuildError
from tokenizer_management.config import VOCAB_PATH

# === Pytest iÃ§in test verilerini hazÄ±rlayan fixture ===
@pytest.fixture
def vocab_builder():
    special_tokens = {
        "<PAD>": 0,
        "<UNK>": 1,
        "<EOS>": 2,
        "<BOS>": 3
    }
    return VocabBuilder(special_tokens=special_tokens)

# === VOCAB_PATH dosyasÄ±nÄ± yedekleyen fixture ===
@pytest.fixture(scope="function", autouse=True)
def setup_and_teardown_vocab():
    backup_path = VOCAB_PATH + ".bak"
    if os.path.exists(VOCAB_PATH):
        os.rename(VOCAB_PATH, backup_path)

    yield
    
    if os.path.exists(backup_path):
        if os.path.exists(VOCAB_PATH):
            os.remove(VOCAB_PATH)
        os.rename(backup_path, VOCAB_PATH)

# === ğŸ§ª Testler ===

# âœ… 1ï¸âƒ£ BaÅŸarÄ±lÄ± bir ÅŸekilde liste ile vocab oluÅŸturma
def test_build_from_list(vocab_builder):
    token_list = ["merhaba", "dÃ¼nya", "merhaba", "cevahir"]

    # Vocab oluÅŸtur
    vocab_builder.build_from_list(token_list)

    # DoÄŸru tokenler var mÄ± kontrol et
    vocab = vocab_builder.vocab
    assert vocab["merhaba"] is not None
    assert vocab["dÃ¼nya"] is not None
    assert vocab["cevahir"] is not None

    # ID'ler unique olmalÄ±
    assert len(set(vocab.values())) == len(vocab)

    # Vocab boyutu doÄŸru mu?
    assert vocab_builder.get_vocab_size() == 7


# âœ… 2ï¸âƒ£ BaÅŸarÄ±lÄ± bir ÅŸekilde metinden vocab oluÅŸturma
def test_build_from_text(vocab_builder):
    text = "merhaba dÃ¼nya merhaba cevahir merhaba"
    
    # Vocab oluÅŸtur
    vocab_builder.build_from_text(text, min_frequency=2)

    vocab = vocab_builder.vocab

    assert "merhaba" in vocab
    assert "dÃ¼nya" not in vocab   # min_frequency = 2 olduÄŸu iÃ§in eklenmemeli
    assert "cevahir" not in vocab # min_frequency = 2 olduÄŸu iÃ§in eklenmemeli

    # Vocab boyutu kontrolÃ¼
    assert vocab_builder.get_vocab_size() == 5


# âœ… 3ï¸âƒ£ Token ID Ã§akÄ±ÅŸma hatasÄ± kontrolÃ¼
def test_token_id_conflict(vocab_builder):
    vocab_builder.add_token("token_1", token_id=10)

    # AynÄ± ID ile baÅŸka bir token eklemeye Ã§alÄ±ÅŸÄ±nca hata vermeli
    with pytest.raises(VocabBuildError, match="Token ID Ã§akÄ±ÅŸmasÄ±"):
        vocab_builder.add_token("token_2", token_id=10)


# âœ… 4ï¸âƒ£ GeÃ§ersiz string formatÄ± hatasÄ± kontrolÃ¼
def test_invalid_text_input(vocab_builder):
    with pytest.raises(TypeError, match="Girdi metni bir string olmalÄ±dÄ±r"):
        vocab_builder.build_from_text(12345)


# âœ… 5ï¸âƒ£ GeÃ§ersiz liste formatÄ± hatasÄ± kontrolÃ¼
def test_invalid_list_input(vocab_builder):
    with pytest.raises(TypeError, match="Girdi bir liste olmalÄ±dÄ±r"):
        vocab_builder.build_from_list("Bu bir liste deÄŸil")


# âœ… 6ï¸âƒ£ BaÅŸarÄ±lÄ± bir ÅŸekilde vocab dosyasÄ±nÄ± kaydetme
def test_save_vocab(vocab_builder):
    token_list = ["merhaba", "dÃ¼nya", "cevahir"]
    vocab_builder.build_from_list(token_list)
    vocab_builder.save_vocab()

    assert os.path.exists(VOCAB_PATH)

    # JSON formatÄ± doÄŸru mu kontrol et
    with open(VOCAB_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
        assert data["merhaba"] is not None
        assert data["dÃ¼nya"] is not None
        assert data["cevahir"] is not None


# âœ… 7ï¸âƒ£ Dosya yazma hatasÄ± kontrolÃ¼
def test_save_vocab_permission_error(vocab_builder, mocker):
    mocker.patch("builtins.open", side_effect=PermissionError("Ä°zin hatasÄ±"))

    with pytest.raises(VocabBuildError, match="Vocab dosyasÄ± yazÄ±lamadÄ±"):
        vocab_builder.save_vocab()


# âœ… 8ï¸âƒ£ Token tekrar etme kontrolÃ¼
def test_add_duplicate_token(vocab_builder):
    vocab_builder.add_token("token_1")
    
    # Tekrar token eklediÄŸinde uyarÄ± vermeli ama exception atmamalÄ±
    vocab_builder.add_token("token_1")
    vocab = vocab_builder.vocab

    assert vocab["token_1"] is not None
    assert len(vocab) == 5  # Ã–zel tokenlar + token_1


# âœ… 9ï¸âƒ£ Vocab boyutunu kontrol etme
def test_get_vocab_size(vocab_builder):
    token_list = ["merhaba", "dÃ¼nya", "cevahir"]
    vocab_builder.build_from_list(token_list)

    size = vocab_builder.get_vocab_size()
    assert size == 7


# âœ… ğŸ”¥ EKSTRA TEST ğŸ”¥: BoÅŸ metin giriÅŸi
def test_build_from_empty_text(vocab_builder):
    vocab_builder.build_from_text("")
    vocab = vocab_builder.vocab

    assert len(vocab) == 4  # Sadece Ã¶zel tokenler olmalÄ±


# âœ… ğŸ”¥ EKSTRA TEST ğŸ”¥: BoÅŸ liste giriÅŸi
def test_build_from_empty_list(vocab_builder):
    vocab_builder.build_from_list([])
    vocab = vocab_builder.vocab

    assert len(vocab) == 4  # Sadece Ã¶zel tokenler olmalÄ±


# âœ… ğŸ”¥ EKSTRA TEST ğŸ”¥: Ã‡ift token ID hatasÄ± testi
def test_duplicate_token_id(vocab_builder):
    vocab_builder.add_token("token_1", 10)
    with pytest.raises(VocabBuildError, match="Token ID Ã§akÄ±ÅŸmasÄ±"):
        vocab_builder.add_token("token_2", 10)

